{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "proud-tulsa",
    "outputId": "4d3132b6-fe5a-43c7-ead8-f623d943b967"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#jsonモジュールのインポート\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joint-stevens"
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GTWCHmkrhOb",
    "outputId": "e2a4ebb9-c526-4f72-d491-3b4400ca981f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R31PrYcerVho"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/drive/My Drive/Colab Notebooks/CommonLit Readability/test.csv\")\n",
    "train_df = pd.read_csv(\"/drive/My Drive/Colab Notebooks/CommonLit Readability/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwMbfM-xGBVv"
   },
   "source": [
    "##Baseline Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "defensive-consumption"
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = ''.join([k if k not in string.punctuation else ' ' for k in text])\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "charged-aircraft"
   },
   "outputs": [],
   "source": [
    "train_df['excerpt'] = train_df['excerpt'].apply(text_cleaning)\n",
    "test_df['excerpt'] = test_df['excerpt'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "desirable-assault"
   },
   "outputs": [],
   "source": [
    "def create_taggedDocument_from_text(row):\n",
    "    text = row['excerpt']\n",
    "    #text = remove_stopwords(text)\n",
    "    \n",
    "    textWordlist = nltk.word_tokenize(text)\n",
    "\n",
    "    wordlist = [word for word in textWordlist]\n",
    "    #wordlist = [snowball.stem(word) for word in textWordlist]\n",
    "    return TaggedDocument(words=wordlist, tags=[row['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QcXH0UMuKpU",
    "outputId": "2aa062f3-e0a0-4635-cdb8-2e66dbb9ee86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "operating-cream"
   },
   "outputs": [],
   "source": [
    "train_df['taggedDocument'] = train_df.apply(create_taggedDocument_from_text, axis=1)\n",
    "test_df['taggedDocument'] = test_df.apply(create_taggedDocument_from_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solar-mixture"
   },
   "outputs": [],
   "source": [
    "training_docs = train_df['taggedDocument'].values.tolist() + test_df['taggedDocument'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "italian-classroom"
   },
   "outputs": [],
   "source": [
    "# 学習実行（パラメータを調整可能）\n",
    "# documents:学習データ（TaggedDocumentのリスト）\n",
    "# min_count=1:最低1回出現した単語を学習に使用する\n",
    "# dm=0:学習モデル=DBOW（デフォルトはdm=1:学習モデル=DM）\n",
    "dvmodel = Doc2Vec(documents=training_docs, \n",
    "                epochs=50, \n",
    "                alpha=0.0025, \n",
    "                min_alpha=0.000001, \n",
    "                sample=0.001, \n",
    "                min_count=5, \n",
    "                window=15, \n",
    "                negative=5,\n",
    "                ns_exponent=0.75, \n",
    "                dbow_words=0, \n",
    "                dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frequent-integer"
   },
   "outputs": [],
   "source": [
    "#Doc2Vecからベクトルを特徴量として抽出\n",
    "train_docvecs_df = pd.DataFrame()\n",
    "test_docvecs_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for Id in train_df[\"id\"]:\n",
    "    train_docvecs_df[Id] = dvmodel.dv[Id]\n",
    "for Id in test_df[\"id\"]:\n",
    "    test_docvecs_df[Id] = dvmodel.dv[Id]\n",
    "\n",
    "train_docvecs_df = train_docvecs_df.T\n",
    "train_docvecs_df = train_docvecs_df.rename_axis('id').reset_index()\n",
    "\n",
    "test_docvecs_df = test_docvecs_df.T\n",
    "test_docvecs_df = test_docvecs_df.rename_axis('id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "descending-values"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_docvecs_df.drop('id', axis=1), train_df['target'], test_size = 0.3, random_state=71)\n",
    "lgb_train = lgb.Dataset(train_X.values, train_y.values)\n",
    "lgb_eval = lgb.Dataset(val_X.values, val_y.values, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "canadian-thread",
    "outputId": "e716a00b-9d92-4feb-f9d0-ae4eae2550f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's rmse: 0.860603\n",
      "[200]\tvalid_0's rmse: 0.867301\n",
      "[300]\tvalid_0's rmse: 0.868667\n",
      "[400]\tvalid_0's rmse: 0.869191\n",
      "[500]\tvalid_0's rmse: 0.869261\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's rmse: 0.841667\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    # 回帰問題\n",
    "    'objective': 'regression',\n",
    "    # RMSEで評価\n",
    "    'metric': 'rmse',\n",
    "}\n",
    "lgbModel = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                     verbose_eval=100,  # 50イテレーション毎に学習結果出力\n",
    "                     num_boost_round=1000,  # 最大イテレーション回数指定\n",
    "                     early_stopping_rounds=500,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "compliant-profit",
    "outputId": "6d70641f-5c67-40d2-bbbf-466549ed83aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8416665319177608"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lgbModel.predict(val_X.values, num_iteration=lgbModel.best_iteration)\n",
    "rmse_baseline = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beautiful-bobby"
   },
   "outputs": [],
   "source": [
    "predicted = lgbModel.predict(test_docvecs_df.drop('id', axis=1).values, num_iteration=lgbModel.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smart-demand",
    "outputId": "2a8ef26c-9b2d-4437-f7e0-e5ff2eb554ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31582229, -0.14363317, -0.2113018 , -1.35768384, -1.96663057,\n",
       "       -0.71837581, -0.3442394 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe6JJ2zrGgIV"
   },
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "no6r-Q0NHmGS"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2h_-NAEhD7l"
   },
   "outputs": [],
   "source": [
    "#LightGBM without parameter tuning\n",
    "lgModel2 = lgb.LGBMRegressor().fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMGZvUlrhqpe",
    "outputId": "d8e1f4d3-1021-4ed4-bb8c-756f6feca805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8357415269874051"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lgModel2.predict(val_X.values)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfqaD0PP0tBf"
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lrModel = LinearRegression().fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDGV16xx6fU-",
    "outputId": "f0acc64f-4c5b-448f-c45b-b836b6a356a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8635875226486939"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lrModel.predict(val_X.values)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIsvHttuKaaR"
   },
   "outputs": [],
   "source": [
    "#Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "rrModel = Ridge().fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSBjCQurfB7v",
    "outputId": "f086832a-8170-49a0-e82f-b995ae4cd647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8559836986869808"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rrModel.predict(val_X.values)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIxaCpaKfFfO"
   },
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "dtModel = DecisionTreeRegressor().fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpmspILTfkaF",
    "outputId": "dd9be04f-3a32-4981-988c-ee233d2d2bef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.124067296111009"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dtModel.predict(val_X.values)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRseGpKlfu1s"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfModel = RandomForestRegressor().fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPV4AYfvg42m",
    "outputId": "a17cbfd2-10ab-497d-9f59-73fcd01db9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355171932119455"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfModel.predict(val_X.values)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQU27TPIrz-j"
   },
   "source": [
    "Random Forest Parameter Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0FPGFb4r6jK",
    "outputId": "54df17c6-337c-46db-a0d7-43169ef00bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "\n",
    "rfModel2 = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfModel2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ftgl5YFsNXA",
    "outputId": "ab165d51-e86f-4217-cda8-3e2eda18e360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLIO2G2sgXV",
    "outputId": "d9789068-ee97-4992-ffcb-0e2d3dc74c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 21.5min\n",
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 94.5min\n"
     ]
    }
   ],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rfModel2, param_distributions = random_grid, scoring=\"neg_mean_squared_error\", n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1chVqonOtHBf",
    "outputId": "c5e480d6-abd1-4804-9472-c95582cdd62c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 800}"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmxtCJFhie3n",
    "outputId": "ada6a703-995e-4c89-965e-8b6a1e7fe4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Base:  0.8731974651457542\n",
      "RMSE Random:  0.8242243824779221\n",
      "Improvement of 5.61%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(train_X.values, train_y.values)\n",
    "pred_base = base_model.predict(val_X.values)\n",
    "rmse_base = np.sqrt(metrics.mean_squared_error(val_y.values, pred_base))\n",
    "print(\"RMSE Base: \",rmse_base)\n",
    "\n",
    "best_random = RandomForestRegressor(bootstrap = False, max_depth = 30, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 10, n_estimators = 800, random_state = 42)\n",
    "best_random.fit(train_X.values, train_y.values)\n",
    "pred_random = best_random.predict(val_X.values)\n",
    "rmse_random = np.sqrt(metrics.mean_squared_error(val_y.values, pred_random))\n",
    "print(\"RMSE Random: \",rmse_random)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (rmse_base - rmse_random) / rmse_base))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KwTKsD4mh0M",
    "outputId": "356dad1d-9048-47cb-f8cc-d5be57858d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [False],\n",
      " 'max_depth': [30, 40, 50, 60, None],\n",
      " 'max_features': ['sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 3],\n",
      " 'min_samples_split': [8, 10, 12],\n",
      " 'n_estimators': [600, 700, 800, 900, 1000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [30, 40, 50, 60, None],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [600, 700, 800, 900, 1000]\n",
    "}\n",
    "\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfT8YQHB1rpi",
    "outputId": "1c99db38-edee-4602-8819-d0db9a31f70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 225 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 39.2min\n"
     ]
    }
   ],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(train_X.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWfJxuyqyjxW",
    "outputId": "d3cbf613-d81d-4fc2-f57b-4c8be41e78a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 12,\n",
       " 'n_estimators': 700}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eCinZTsyrSl",
    "outputId": "0cdb4f4e-9cc3-4ade-845f-ecc42af4cca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Base:  0.8661309124647412\n",
      "RMSE Grid:  0.8239196044889792\n",
      "Improvement of 4.87%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(train_X.values, train_y.values)\n",
    "pred_base = base_model.predict(val_X.values)\n",
    "rmse_base = np.sqrt(metrics.mean_squared_error(val_y.values, pred_base))\n",
    "print(\"RMSE Base: \",rmse_base)\n",
    "\n",
    "best_grid = RandomForestRegressor(bootstrap = False, max_depth = None, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 12, n_estimators = 700, random_state = 42)\n",
    "best_grid.fit(train_X.values, train_y.values)\n",
    "pred_grid = best_grid.predict(val_X.values)\n",
    "rmse_grid = np.sqrt(metrics.mean_squared_error(val_y.values, pred_grid))\n",
    "print(\"RMSE Grid: \",rmse_grid)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (rmse_base - rmse_grid) / rmse_base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBDlr4PLMqZ3",
    "outputId": "7964ded0-806b-450b-cee1-21fd8b07157b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Baseline:  0.8416665319177608\n",
      "RMSE Improved Model:  0.824183877763254\n",
      "Improvement of 2.08%.\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE Baseline: \",rmse_baseline)\n",
    "print(\"RMSE Improved Model: \",rmse_grid)\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (rmse_baseline - rmse_grid) / rmse_baseline))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "commonlit-doc2vec-lightgbm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 52.734064,
   "end_time": "2021-05-13T13:18:47.283152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-13T13:17:54.549088",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
